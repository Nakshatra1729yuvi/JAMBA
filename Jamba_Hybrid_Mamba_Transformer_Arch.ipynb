{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p3vRHoNgXDLb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import einsum,rearrange,repeat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridConfig:\n",
        "  def __init__(self):\n",
        "    self.vocab_size=50257\n",
        "    self.n_head=8\n",
        "    self.n_embd=1024\n",
        "    self.block_size=4048\n",
        "    self.dropout=0.1\n",
        "    self.n_intmd=2048\n",
        "    self.ssm_hid=64\n",
        "    self.kernel=4\n",
        "    self.num_layers=3\n",
        "    self.ssm_delta=64\n",
        "    self.n_exp=16\n",
        "    self.k=2"
      ],
      "metadata": {
        "id": "VmXOpICAX5pr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    assert config.n_embd%config.n_head==0\n",
        "    self.n_embd=config.n_embd\n",
        "    self.n_head=config.n_head\n",
        "    self.attn_proj=nn.Linear(config.n_embd,3*config.n_embd)\n",
        "    self.c_proj=nn.Linear(config.n_embd,config.n_embd)\n",
        "    self.d_k=self.n_embd//self.n_head\n",
        "    self.register_buffer('bias',torch.tril(torch.ones((1,1,config.block_size,config.block_size))))\n",
        "    self.attn_dropout=nn.Dropout(config.dropout)\n",
        "    self.resid_dropout=nn.Dropout(config.dropout)\n",
        "  def forward(self,x):\n",
        "    B,T,C=x.shape\n",
        "    q,k,v=self.attn_proj(x).split(self.n_embd,dim=-1)\n",
        "    q=q.view(B,T,self.n_head,self.d_k).transpose(1,2)\n",
        "    k=k.view(B,T,self.n_head,self.d_k).transpose(1,2)\n",
        "    v=v.view(B,T,self.n_head,self.d_k).transpose(1,2)\n",
        "    attn=(q@k.transpose(-1,-2))*(1/(math.sqrt(self.d_k)))\n",
        "    attn=attn.masked_fill(self.bias[:,:,:T,:T]==0,float('-inf'))\n",
        "    attn=F.softmax(attn,dim=-1)\n",
        "    attn=self.attn_dropout(attn)\n",
        "\n",
        "    y=attn@v\n",
        "    y=y.transpose(1,2).contiguous().view(B,T,C)\n",
        "    return self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3OooCGXXxKG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mamba(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.ssm_hid=config.ssm_hid\n",
        "    self.intermediate_size=config.n_intmd\n",
        "    self.delta_rank=config.ssm_delta\n",
        "\n",
        "    self.in_proj=nn.Linear(config.n_embd,config.n_intmd*2)\n",
        "\n",
        "    self.conv=nn.Conv1d(in_channels=config.n_intmd,out_channels=config.n_intmd,kernel_size=config.kernel,padding=config.kernel-1,groups=config.n_intmd)\n",
        "\n",
        "    self.x_proj=nn.Linear(config.n_intmd,config.ssm_delta+2*config.ssm_hid)\n",
        "    self.delta_proj=nn.Linear(config.ssm_delta,config.n_intmd)\n",
        "\n",
        "    self.A=repeat((torch.arange(config.ssm_hid)),'n -> d n',d=config.n_intmd)\n",
        "    self.A_log=nn.Parameter(torch.log(self.A))\n",
        "    self.D=nn.Parameter(torch.ones(config.n_intmd))\n",
        "\n",
        "    self.out_proj=nn.Linear(config.n_intmd,config.n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_len,_=x.shape\n",
        "    xz=self.in_proj(x)\n",
        "    xz=rearrange(xz,\"b l x -> b x l\")\n",
        "    x,z=xz.chunk(2,dim=1)\n",
        "\n",
        "    x=self.conv(x)[:,:,:seq_len]\n",
        "    x=F.silu(x)\n",
        "\n",
        "    y=self.ssm(x)\n",
        "\n",
        "    y=y*F.silu(z)\n",
        "\n",
        "    output=self.out_proj(rearrange(y,\"b d l -> b l d\"))\n",
        "\n",
        "    return output\n",
        "    print(output.shape,z.shape)\n",
        "\n",
        "  def ssm(self,x):\n",
        "\n",
        "    A=-torch.exp(self.A_log.float())\n",
        "    D=self.D.float()\n",
        "\n",
        "    x_rearrange=rearrange(x,'b d l -> b l d')\n",
        "    x_rearrange=self.x_proj(x_rearrange)\n",
        "    delta,B,C=x_rearrange.split([self.delta_rank,self.ssm_hid,self.ssm_hid],dim=-1)\n",
        "    delta=F.softplus(self.delta_proj(delta))\n",
        "\n",
        "    y=self.selective_scan(x,A,B,C,D,delta)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def selective_scan(self,u,A,B,C,D,delta):\n",
        "    b,d_in,l=u.shape\n",
        "    n=A.shape[1]\n",
        "    deltaA=torch.exp(einsum(delta,A,'b l d_in , d_in n -> b d_in l n'))\n",
        "    deltaB_u=einsum(delta,B,u,'b l d_in , b l n , b d_in l -> b d_in l n')\n",
        "    x=torch.zeros((b,d_in,n),device=next(self.parameters()).device)\n",
        "    ys=[]\n",
        "    for i in range(l):\n",
        "      x=deltaA[:,:,i]*x+deltaB_u[:,:,i]\n",
        "      y=einsum(x,C[:,i,:],'b d_in n , b n -> b d_in')\n",
        "      ys.append(y)\n",
        "    y=torch.stack(ys,dim=2)\n",
        "\n",
        "    y=y+u*rearrange(D,'d_in -> d_in 1')\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "K-gB0yv8YlC5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.up_proj=nn.Linear(config.n_embd,config.n_embd*4)\n",
        "    self.gate_proj=nn.Linear(config.n_embd,config.n_embd*4)\n",
        "    self.down_proj=nn.Linear(config.n_embd*4,config.n_embd)\n",
        "  def forward(self,x):\n",
        "    return self.down_proj(self.up_proj(x)*F.silu(self.gate_proj(x)))"
      ],
      "metadata": {
        "id": "x5GyrTC_Y_Bs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MOE(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.n_experts=config.n_exp\n",
        "    self.k=config.k\n",
        "    self.gate_proj=nn.Linear(config.n_embd,config.n_exp)\n",
        "    self.experts=nn.ModuleList([MLP(config) for _ in range(config.n_exp)])\n",
        "  def forward(self,x):\n",
        "    B,T,C=x.shape\n",
        "    x_flat=x.reshape(-1,C)\n",
        "    gate_score=self.gate_proj(x_flat)\n",
        "    outputs=torch.zeros_like(x_flat)\n",
        "    top_val,top_idx=torch.topk(gate_score,self.k)\n",
        "    for i in range(self.k):\n",
        "      expert_idx=top_idx[:,i]\n",
        "      expert_val=top_val[:,i]\n",
        "      for e in range(self.n_experts):\n",
        "        mask=(expert_idx==e)\n",
        "        if mask.sum()==0:\n",
        "          continue\n",
        "        out=self.experts[e](x_flat[mask])\n",
        "        outputs[mask]+=out*expert_val[mask].unsqueeze(-1)\n",
        "    return outputs.reshape(B,T,C)\n"
      ],
      "metadata": {
        "id": "vBTGICkVZMVG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn_Block(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.rn1=nn.RMSNorm(config.n_embd)\n",
        "    self.rn2=nn.RMSNorm(config.n_embd)\n",
        "    self.attn=MHA(config)\n",
        "    self.ff=MLP(config)\n",
        "  def forward(self,x):\n",
        "    x=x+self.attn(self.rn1(x))\n",
        "    x=x+self.ff(self.rn2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "END3ZvZcZB92"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn_MOE_Block(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.rn1=nn.RMSNorm(config.n_embd)\n",
        "    self.rn2=nn.RMSNorm(config.n_embd)\n",
        "    self.attn=MHA(config)\n",
        "    self.ff=MOE(config)\n",
        "  def forward(self,x):\n",
        "    x=x+self.attn(self.rn1(x))\n",
        "    x=x+self.ff(self.rn2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "MS5X4VNkfoT4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mamba_Block(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.rn1=nn.RMSNorm(config.n_embd)\n",
        "    self.rn2=nn.RMSNorm(config.n_embd)\n",
        "    self.mamba=Mamba(config)\n",
        "    self.ff=MLP(config)\n",
        "  def forward(self,x):\n",
        "    x=x+self.mamba(self.rn1(x))\n",
        "    x=x+self.ff(self.rn2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "D-7oQ9phfpBg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mamba_MOE_Block(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.rn1=nn.RMSNorm(config.n_embd)\n",
        "    self.rn2=nn.RMSNorm(config.n_embd)\n",
        "    self.mamba=Mamba(config)\n",
        "    self.ff=MOE(config)\n",
        "  def forward(self,x):\n",
        "    x=x+self.mamba(self.rn1(x))\n",
        "    x=x+self.ff(self.rn2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "aPi3Oi5UfpnJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eot_token=50257"
      ],
      "metadata": {
        "id": "igOVl0n1Zwn7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridBlock(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        Mamba_Block(config),\n",
        "        Mamba_MOE_Block(config),\n",
        "        Mamba_Block(config),\n",
        "        Mamba_MOE_Block(config),\n",
        "        Attn_Block(config),\n",
        "        Mamba_MOE_Block(config),\n",
        "        Mamba_Block(config),\n",
        "        Mamba_MOE_Block(config)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "63rdoFk7gFKL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Jamba(nn.Module):\n",
        "  def __init__(self,config:HybridConfig):\n",
        "    super().__init__()\n",
        "    self.block_size=config.block_size\n",
        "    self.token_emb=nn.Embedding(config.vocab_size,config.n_embd)\n",
        "    self.pos_emb=nn.Embedding(config.block_size,config.n_embd)\n",
        "    self.drop=nn.Dropout(config.dropout)\n",
        "    self.blocks=nn.ModuleList([HybridBlock(config) for _ in range(config.num_layers)])\n",
        "    self.rn=nn.RMSNorm(config.n_embd)\n",
        "    self.head=nn.Linear(config.n_embd,config.vocab_size)\n",
        "  def forward(self,ids,labels=None):\n",
        "    B,T=ids.shape\n",
        "    assert T<=self.block_size\n",
        "    pos=torch.arange(0,T,dtype=torch.long,device=ids.device).unsqueeze(0)\n",
        "    emb_ids=self.token_emb(ids)+self.pos_emb(pos)\n",
        "    x=self.drop(emb_ids)\n",
        "\n",
        "    for blk in self.blocks:\n",
        "      x=blk(x)\n",
        "\n",
        "    x=self.rn(x)\n",
        "    logits=self.head(x)\n",
        "    loss=None\n",
        "    if labels is not None:\n",
        "      loss=F.cross_entropy(logits.view(-1,logits.size(-1)),labels.view(-1))\n",
        "    return logits,loss\n",
        "  @torch.no_grad()\n",
        "  def generate(self,idx,max_tokens=50,temprature=0.8,topk=None):\n",
        "    for _ in range(max_tokens):\n",
        "      idx=idx[:,-self.block_size:]\n",
        "      logits,_=self(idx)\n",
        "      logit=logits[:,-1,:]/temprature\n",
        "      if topk is not None:\n",
        "        v,i=torch.topk(logit,topk)\n",
        "        mask=logit<v[:,-1].unsqueeze(-1)\n",
        "        mask=mask.to(logit.device)\n",
        "        logit[mask]=float(\"-inf\")\n",
        "      prob=F.softmax(logit,dim=-1)\n",
        "      next_token=torch.multinomial(prob,num_samples=1)\n",
        "      idx=torch.concat([idx,next_token],dim=-1)\n",
        "      if next_token.item()==eot_token:\n",
        "        break\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "09vCRqAcZHSw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=HybridConfig()\n",
        "model=Jamba(c)"
      ],
      "metadata": {
        "id": "yhe_oL3XhOiY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  trainable_parameters=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total Trainable Parameters:{trainable_parameters:,}\")"
      ],
      "metadata": {
        "id": "qy6VhD8biJFa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ftk1_rDijsQ",
        "outputId": "b66018a0-d5e5-4bc1-c649-5122eed099fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Trainable Parameters:2,835,053,777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(torch.randint(0,50000,[1,5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8psnT4thuIM",
        "outputId": "1c174593-826d-47d3-fab8-17d5ffba1c3e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20789, 38886, 48958,  6831,  9951, 13429, 27901, 38441, 22170, 22162,\n",
              "         33327, 16110, 28790, 16544,   904, 36708, 13009, 12915, 31194, 15035,\n",
              "         15046,  3260,  4580,  5893, 34514, 42128, 26801, 22573, 44944, 36072,\n",
              "         26795,  3584, 30322, 24899, 43050, 43590, 36390, 26537, 21126, 29833,\n",
              "          3531, 36527, 33205, 10579, 37464, 48332, 12284, 18395, 35520, 39776,\n",
              "         46731, 24171, 33897, 37165, 24414]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}